{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SimilarityRu.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "teVN4SM5Shzl",
        "colab_type": "code",
        "outputId": "58dea9a1-3e06-4b50-dfb9-1898ba195e4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        }
      },
      "source": [
        "!pip install fasttext pyonmttok"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fasttext\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/61/2e01f1397ec533756c1d893c22d9d5ed3fce3a6e4af1976e0d86bb13ea97/fasttext-0.9.1.tar.gz (57kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 3.2MB/s \n",
            "\u001b[?25hCollecting pyonmttok\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/0c/5d36e4d5c9cf7e8ec40dcf59a08cfc1a0ec55f7e729a9457ae9bff4f2d4a/pyonmttok-1.17.2-cp36-cp36m-manylinux1_x86_64.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2MB 13.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.6/dist-packages (from fasttext) (2.4.3)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from fasttext) (42.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fasttext) (1.17.4)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.1-cp36-cp36m-linux_x86_64.whl size=2385238 sha256=c657220ffc85481c8382409f93e574c7a034ded868cfebadf4820450b4e82e8c\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/f0/04/caa82c912aee89ce76358ff954f3f0729b7577c8ff23a292e3\n",
            "Successfully built fasttext\n",
            "Installing collected packages: fasttext, pyonmttok\n",
            "Successfully installed fasttext-0.9.1 pyonmttok-1.17.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZEkT7r6YsOD",
        "colab_type": "code",
        "outputId": "de71158e-e4c7-46f0-e204-744a570b5f61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install --upgrade keras tensorflow-gpu"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/fd/6bfe87920d7f4fd475acd28500a42482b6b84479832bdc0fe9e589a60ceb/Keras-2.3.1-py2.py3-none-any.whl (377kB)\n",
            "\u001b[K     |████████████████████████████████| 378kB 4.7MB/s \n",
            "\u001b[?25hCollecting tensorflow-gpu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/44/47f0722aea081697143fbcf5d2aa60d1aee4aaacb5869aee2b568974777b/tensorflow_gpu-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (380.8MB)\n",
            "\u001b[K     |████████████████████████████████| 380.8MB 42kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.3.3)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.8.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.17.4)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.8)\n",
            "Requirement already satisfied, skipping upgrade: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.2.2)\n",
            "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.1.8)\n",
            "Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.1.0)\n",
            "Collecting tensorflow-estimator<2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/08/8b927337b7019c374719145d1dceba21a8bb909b93b1ad6f8fb7d22c1ca1/tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 41.1MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.33.6)\n",
            "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.11.2)\n",
            "Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.8.1)\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.8.1)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.10.0)\n",
            "Collecting tensorboard<2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/54/99b9d5d52d5cb732f099baaaf7740403e83fe6b0cedde940fabd2b13d75a/tensorboard-2.0.2-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 36.4MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu) (42.0.2)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (3.1.1)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (2.21.0)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (0.16.0)\n",
            "Collecting google-auth<2,>=1.6.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/f8/84b5771faec3eba9fe0c91c8c5896364a8ba08852c0dea5ad2025026dd95/google_auth-1.10.0-py2.py3-none-any.whl (76kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 11.7MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (2.8)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (2019.11.28)\n",
            "Requirement already satisfied, skipping upgrade: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (4.0)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (4.0.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (0.2.7)\n",
            "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu) (0.4.8)\n",
            "\u001b[31mERROR: tensorflow 1.15.0 has requirement tensorboard<1.16.0,>=1.15.0, but you'll have tensorboard 2.0.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 1.15.0 has requirement tensorflow-estimator==1.15.1, but you'll have tensorflow-estimator 2.0.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorboard 2.0.2 has requirement grpcio>=1.24.3, but you'll have grpcio 1.15.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement google-auth~=1.4.0, but you'll have google-auth 1.10.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: keras, tensorflow-estimator, google-auth, tensorboard, tensorflow-gpu\n",
            "  Found existing installation: Keras 2.2.5\n",
            "    Uninstalling Keras-2.2.5:\n",
            "      Successfully uninstalled Keras-2.2.5\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "  Found existing installation: google-auth 1.4.2\n",
            "    Uninstalling google-auth-1.4.2:\n",
            "      Successfully uninstalled google-auth-1.4.2\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "Successfully installed google-auth-1.10.0 keras-2.3.1 tensorboard-2.0.2 tensorflow-estimator-2.0.1 tensorflow-gpu-2.0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwDpQGn4SLL1",
        "colab_type": "code",
        "outputId": "cac67674-76ad-49c1-d456-d392a11d8010",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 559
        }
      },
      "source": [
        "!rm -f ru_tg_train.tar.gz\n",
        "!wget https://www.dropbox.com/s/1ecl9orr2tagcgi/ru_tg_train.tar.gz\n",
        "!rm -f ru_tg_train.json\n",
        "!tar -xzvf ru_tg_train.tar.gz\n",
        "!rm ru_tg_train.tar.gz"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-01-02 14:28:53--  https://www.dropbox.com/s/tjai0ztikym9km1/ru_tg_train.tar.gz\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.1.1, 2620:100:6016:1::a27d:101\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.1.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/tjai0ztikym9km1/ru_tg_train.tar.gz [following]\n",
            "--2020-01-02 14:28:53--  https://www.dropbox.com/s/raw/tjai0ztikym9km1/ru_tg_train.tar.gz\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc4a76e0280f153b07e07ef8cfd0.dl.dropboxusercontent.com/cd/0/inline/AvahtINnfyIb_Iz9nVU3gDo1_MS0ypKSvg8xyAZ54OLz2zxhuptz6vAY3oYGQ35XEs1-bXOD14sMSqcVLBmA7SiY5fY_tGHlUbTAMqYeb5QDiTckhNm_MMzh63ICjZqBEsw/file# [following]\n",
            "--2020-01-02 14:28:53--  https://uc4a76e0280f153b07e07ef8cfd0.dl.dropboxusercontent.com/cd/0/inline/AvahtINnfyIb_Iz9nVU3gDo1_MS0ypKSvg8xyAZ54OLz2zxhuptz6vAY3oYGQ35XEs1-bXOD14sMSqcVLBmA7SiY5fY_tGHlUbTAMqYeb5QDiTckhNm_MMzh63ICjZqBEsw/file\n",
            "Resolving uc4a76e0280f153b07e07ef8cfd0.dl.dropboxusercontent.com (uc4a76e0280f153b07e07ef8cfd0.dl.dropboxusercontent.com)... 162.125.1.6, 2620:100:601b:6::a27d:806\n",
            "Connecting to uc4a76e0280f153b07e07ef8cfd0.dl.dropboxusercontent.com (uc4a76e0280f153b07e07ef8cfd0.dl.dropboxusercontent.com)|162.125.1.6|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 FOUND\n",
            "Location: /cd/0/inline2/AvbBXg4XWw9nXeE89o4whC-iNoyRb3TZ2KHflIuGO7jBxm2U1NfFQ4AU1PmIvlDZO1zUjoMPPZfTUpREUCN6LwxulG1A87DxLu_qlw25-qy0nkH4UIAXtgAj_51-yfpMhKv6rDiygJ1SduA-n5nqaRR_BvtzM_cbmTMJQWYv7JqPNBFt7M3WUK8dr9pX5ky_vmibMC6PLaj_q9DgRrD5_c4Kk1cBTIsVX7eh_nu9L5sL5UZnP8X7Eb9JYwnrZs-ESq3Q7EVgxdmqleFdd8QGXzrCEDnSbJZ9JZbBQVvXnbcqjOG0nGD8D9lsNnsfMeLuQrkShKYAEzNH-J2JqC07PHSGmuqTVjBLeeT-KEzztGIGeQ/file [following]\n",
            "--2020-01-02 14:28:54--  https://uc4a76e0280f153b07e07ef8cfd0.dl.dropboxusercontent.com/cd/0/inline2/AvbBXg4XWw9nXeE89o4whC-iNoyRb3TZ2KHflIuGO7jBxm2U1NfFQ4AU1PmIvlDZO1zUjoMPPZfTUpREUCN6LwxulG1A87DxLu_qlw25-qy0nkH4UIAXtgAj_51-yfpMhKv6rDiygJ1SduA-n5nqaRR_BvtzM_cbmTMJQWYv7JqPNBFt7M3WUK8dr9pX5ky_vmibMC6PLaj_q9DgRrD5_c4Kk1cBTIsVX7eh_nu9L5sL5UZnP8X7Eb9JYwnrZs-ESq3Q7EVgxdmqleFdd8QGXzrCEDnSbJZ9JZbBQVvXnbcqjOG0nGD8D9lsNnsfMeLuQrkShKYAEzNH-J2JqC07PHSGmuqTVjBLeeT-KEzztGIGeQ/file\n",
            "Reusing existing connection to uc4a76e0280f153b07e07ef8cfd0.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 100684654 (96M) [application/octet-stream]\n",
            "Saving to: ‘ru_tg_train.tar.gz’\n",
            "\n",
            "ru_tg_train.tar.gz  100%[===================>]  96.02M  28.6MB/s    in 3.4s    \n",
            "\n",
            "2020-01-02 14:28:58 (28.6 MB/s) - ‘ru_tg_train.tar.gz’ saved [100684654/100684654]\n",
            "\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.creationtime'\n",
            "tar: Ignoring unknown extended header keyword 'SCHILY.dev'\n",
            "tar: Ignoring unknown extended header keyword 'SCHILY.ino'\n",
            "tar: Ignoring unknown extended header keyword 'SCHILY.nlink'\n",
            "ru_tg_train.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOVRqoR-yIXd",
        "colab_type": "code",
        "outputId": "0a75616c-ebe7-496e-96f1-d38852f857e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        }
      },
      "source": [
        "!rm -f lenta-ru-news.csv.gz\n",
        "!wget https://github.com/yutkin/Lenta.Ru-News-Dataset/releases/download/v1.0/lenta-ru-news.csv.gz\n",
        "!rm -f lenta-ru-news.csv\n",
        "!gzip -d lenta-ru-news.csv.gz"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-01-02 15:03:40--  https://github.com/yutkin/Lenta.Ru-News-Dataset/releases/download/v1.0/lenta-ru-news.csv.gz\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github-production-release-asset-2e65be.s3.amazonaws.com/87156914/0b363e00-0126-11e9-9e3c-e8c235463bd6?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20200102%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20200102T150340Z&X-Amz-Expires=300&X-Amz-Signature=f4bea75783ad1c3f8cd6a19fe132582f65841cc64d5d0c1b0e42e9f03bf95565&X-Amz-SignedHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3Dlenta-ru-news.csv.gz&response-content-type=application%2Foctet-stream [following]\n",
            "--2020-01-02 15:03:40--  https://github-production-release-asset-2e65be.s3.amazonaws.com/87156914/0b363e00-0126-11e9-9e3c-e8c235463bd6?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20200102%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20200102T150340Z&X-Amz-Expires=300&X-Amz-Signature=f4bea75783ad1c3f8cd6a19fe132582f65841cc64d5d0c1b0e42e9f03bf95565&X-Amz-SignedHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3Dlenta-ru-news.csv.gz&response-content-type=application%2Foctet-stream\n",
            "Resolving github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)... 52.216.106.212\n",
            "Connecting to github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)|52.216.106.212|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 527373240 (503M) [application/octet-stream]\n",
            "Saving to: ‘lenta-ru-news.csv.gz’\n",
            "\n",
            "lenta-ru-news.csv.g 100%[===================>] 502.94M  44.1MB/s    in 12s     \n",
            "\n",
            "2020-01-02 15:03:52 (42.9 MB/s) - ‘lenta-ru-news.csv.gz’ saved [527373240/527373240]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcsXXzshSYI3",
        "colab_type": "code",
        "outputId": "a82f59a0-b194-47a5-a2c1-ede5ebaa33a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        }
      },
      "source": [
        "!wget https://www.dropbox.com/s/2nx97d8nzbzusee/ru_vectors_v2.bin"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-01-02 14:29:14--  https://www.dropbox.com/s/2nx97d8nzbzusee/ru_vectors_v2.bin\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.1.1, 2620:100:6016:1::a27d:101\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.1.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/2nx97d8nzbzusee/ru_vectors_v2.bin [following]\n",
            "--2020-01-02 14:29:14--  https://www.dropbox.com/s/raw/2nx97d8nzbzusee/ru_vectors_v2.bin\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc7df42f09d56b21d30ee13e9f38.dl.dropboxusercontent.com/cd/0/inline/Ava2VHqHJGjiDpOIXtPO29ORTh1P8ZpPAYGD8OLMFyDBHQLAfnEDE-HAzYlzZqM7jk1eVKvRFH0e-jezOdw-g_pocQOFUunNTdmK5s7m4FeiZKvzIpJGGfAiwkYj79eboKk/file# [following]\n",
            "--2020-01-02 14:29:14--  https://uc7df42f09d56b21d30ee13e9f38.dl.dropboxusercontent.com/cd/0/inline/Ava2VHqHJGjiDpOIXtPO29ORTh1P8ZpPAYGD8OLMFyDBHQLAfnEDE-HAzYlzZqM7jk1eVKvRFH0e-jezOdw-g_pocQOFUunNTdmK5s7m4FeiZKvzIpJGGfAiwkYj79eboKk/file\n",
            "Resolving uc7df42f09d56b21d30ee13e9f38.dl.dropboxusercontent.com (uc7df42f09d56b21d30ee13e9f38.dl.dropboxusercontent.com)... 162.125.8.6, 2620:100:6016:6::a27d:106\n",
            "Connecting to uc7df42f09d56b21d30ee13e9f38.dl.dropboxusercontent.com (uc7df42f09d56b21d30ee13e9f38.dl.dropboxusercontent.com)|162.125.8.6|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 FOUND\n",
            "Location: /cd/0/inline2/AvZXLTCLb2o0OHQEG6irF81--ozSFiNgnquZbBNgC1JiR2fnjWX5zeBiA2LgCFeFnYAdBXg4dWWkysyoiavPoJ5yd3pMUndTwidaKWAg26YM_a8BuY6eAXebPEKlcWwvrAByDjG2ep42AT1BsXiC38XopicYGkXoHXMdmFFjAasJVkqI4ROmC-ES7qn-3SugVvMHTabm4ecMLROZyGdAWqCylAjdPZYvPBIt5CPcDhozWZU5jIrD99RxEa0Wf66COltHY3M6YL3sNnLOj1jhTsDlx2V3IReOa-F9TEpJ1sNONxKsnARWq5SNKS5bPLc1s9g89aP9jNPPbSTxbdxPMGa90BDnNk_X1ANBVqbkKlj_dA/file [following]\n",
            "--2020-01-02 14:29:15--  https://uc7df42f09d56b21d30ee13e9f38.dl.dropboxusercontent.com/cd/0/inline2/AvZXLTCLb2o0OHQEG6irF81--ozSFiNgnquZbBNgC1JiR2fnjWX5zeBiA2LgCFeFnYAdBXg4dWWkysyoiavPoJ5yd3pMUndTwidaKWAg26YM_a8BuY6eAXebPEKlcWwvrAByDjG2ep42AT1BsXiC38XopicYGkXoHXMdmFFjAasJVkqI4ROmC-ES7qn-3SugVvMHTabm4ecMLROZyGdAWqCylAjdPZYvPBIt5CPcDhozWZU5jIrD99RxEa0Wf66COltHY3M6YL3sNnLOj1jhTsDlx2V3IReOa-F9TEpJ1sNONxKsnARWq5SNKS5bPLc1s9g89aP9jNPPbSTxbdxPMGa90BDnNk_X1ANBVqbkKlj_dA/file\n",
            "Reusing existing connection to uc7df42f09d56b21d30ee13e9f38.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 61627749 (59M) [application/octet-stream]\n",
            "Saving to: ‘ru_vectors_v2.bin’\n",
            "\n",
            "ru_vectors_v2.bin   100%[===================>]  58.77M  43.1MB/s    in 1.4s    \n",
            "\n",
            "2020-01-02 14:29:17 (43.1 MB/s) - ‘ru_vectors_v2.bin’ saved [61627749/61627749]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jS5W4zgaSc2G",
        "colab_type": "code",
        "outputId": "abf65981-7254-4be2-9336-68206c69d46e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import fasttext\n",
        "\n",
        "model = fasttext.load_model('ru_vectors_v2.bin')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhY9_QHcS_1U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "\n",
        "with open(\"ru_tg_train.json\", \"r\") as r:\n",
        "    tg_data = json.load(r)\n",
        "tg_data.sort(key=lambda x: x['timestamp'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NY8YsPZdyPNx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "import re\n",
        "\n",
        "def get_date(url):\n",
        "    dates = re.findall(r\"\\d\\d\\d\\d\\/\\d\\d\\/\\d\\d\", url)\n",
        "    return next(iter(dates), None)\n",
        "\n",
        "with open(\"lenta-ru-news.csv\", \"r\") as r:\n",
        "    next(r)\n",
        "    reader = csv.reader(r, delimiter=',')\n",
        "    lenta_data = []\n",
        "    for row in reader:\n",
        "        url, _, text, _, _ = row\n",
        "        date = get_date(url)\n",
        "        lenta_data.append({\"date\": date, \"text\": text, \"site_name\": \"lenta\"})\n",
        "\n",
        "lenta_data.sort(key=lambda x: x[\"date\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvsECav_0I9D",
        "colab_type": "code",
        "outputId": "c6997058-1532-4769-fae3-fa90f3363646",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(lenta_data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "739351"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6KoPumITgbB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def words_to_embed(model, words):\n",
        "    vectors = [model.get_word_vector(w) for w in words]\n",
        "    norm_vectors = [x / np.linalg.norm(x) for x in vectors]\n",
        "    avg_wv = np.mean(norm_vectors, axis=0)\n",
        "    max_wv = np.max(norm_vectors, axis=0)\n",
        "    min_wv = np.min(norm_vectors, axis=0)\n",
        "    return np.concatenate((avg_wv, max_wv, min_wv))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PVkyLzUTm7m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pyonmttok\n",
        "tokenizer = pyonmttok.Tokenizer(\"conservative\", joiner_annotate=False)\n",
        "\n",
        "def preprocess(text):\n",
        "    text = str(text).strip().replace(\"\\n\", \" \").replace(\"\\xa0\", \" \").lower()\n",
        "    tokens, _ = tokenizer.tokenize(text)\n",
        "    text = \" \".join(tokens)\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTP-GBtXSx6R",
        "colab_type": "code",
        "outputId": "83631efe-aaca-4b78-fc3d-511fd06e877e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_samples(data, count):\n",
        "    last_host_end = {}\n",
        "    samples = []\n",
        "    for count, row in enumerate(data[:count]):\n",
        "        if count % 10000 == 0:\n",
        "            print(count)\n",
        "        \n",
        "        host = row['site_name']\n",
        "        text = preprocess(row['text'])\n",
        "        words = text.split(\" \")\n",
        "        if len(words) < 4:\n",
        "            continue\n",
        "        words = words[:300]\n",
        "            \n",
        "        border = len(words) // 2\n",
        "        begin_words = words[:border]\n",
        "        end_words = words[border:]\n",
        "\n",
        "        left_vector = words_to_embed(model, begin_words)\n",
        "        left_text = \" \".join(begin_words)\n",
        "        right_vector = words_to_embed(model, end_words)\n",
        "        right_text = \" \".join(end_words)\n",
        "\n",
        "        samples.append((left_vector, right_vector, left_text, right_text, 1))\n",
        "        if host in last_host_end:\n",
        "            samples.append((left_vector, last_host_end[host][0], left_text, last_host_end[host][1], 0))\n",
        "        last_host_end[host] = (right_vector, right_text)\n",
        "    return samples\n",
        "\n",
        "tg_samples = get_samples(tg_data, 100000)\n",
        "lenta_samples = get_samples(lenta_data, 100000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "5000\n",
            "6000\n",
            "7000\n",
            "8000\n",
            "9000\n",
            "10000\n",
            "11000\n",
            "12000\n",
            "13000\n",
            "14000\n",
            "15000\n",
            "16000\n",
            "17000\n",
            "18000\n",
            "19000\n",
            "20000\n",
            "21000\n",
            "22000\n",
            "23000\n",
            "24000\n",
            "25000\n",
            "26000\n",
            "27000\n",
            "28000\n",
            "29000\n",
            "30000\n",
            "31000\n",
            "32000\n",
            "33000\n",
            "34000\n",
            "35000\n",
            "36000\n",
            "37000\n",
            "38000\n",
            "39000\n",
            "40000\n",
            "41000\n",
            "42000\n",
            "43000\n",
            "44000\n",
            "45000\n",
            "46000\n",
            "47000\n",
            "48000\n",
            "49000\n",
            "50000\n",
            "51000\n",
            "52000\n",
            "53000\n",
            "54000\n",
            "55000\n",
            "56000\n",
            "57000\n",
            "58000\n",
            "59000\n",
            "60000\n",
            "61000\n",
            "62000\n",
            "63000\n",
            "64000\n",
            "65000\n",
            "66000\n",
            "67000\n",
            "68000\n",
            "69000\n",
            "70000\n",
            "71000\n",
            "72000\n",
            "73000\n",
            "74000\n",
            "75000\n",
            "76000\n",
            "77000\n",
            "78000\n",
            "79000\n",
            "80000\n",
            "81000\n",
            "82000\n",
            "83000\n",
            "84000\n",
            "85000\n",
            "86000\n",
            "87000\n",
            "88000\n",
            "89000\n",
            "90000\n",
            "91000\n",
            "92000\n",
            "93000\n",
            "94000\n",
            "95000\n",
            "96000\n",
            "97000\n",
            "98000\n",
            "99000\n",
            "0\n",
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "5000\n",
            "6000\n",
            "7000\n",
            "8000\n",
            "9000\n",
            "10000\n",
            "11000\n",
            "12000\n",
            "13000\n",
            "14000\n",
            "15000\n",
            "16000\n",
            "17000\n",
            "18000\n",
            "19000\n",
            "20000\n",
            "21000\n",
            "22000\n",
            "23000\n",
            "24000\n",
            "25000\n",
            "26000\n",
            "27000\n",
            "28000\n",
            "29000\n",
            "30000\n",
            "31000\n",
            "32000\n",
            "33000\n",
            "34000\n",
            "35000\n",
            "36000\n",
            "37000\n",
            "38000\n",
            "39000\n",
            "40000\n",
            "41000\n",
            "42000\n",
            "43000\n",
            "44000\n",
            "45000\n",
            "46000\n",
            "47000\n",
            "48000\n",
            "49000\n",
            "50000\n",
            "51000\n",
            "52000\n",
            "53000\n",
            "54000\n",
            "55000\n",
            "56000\n",
            "57000\n",
            "58000\n",
            "59000\n",
            "60000\n",
            "61000\n",
            "62000\n",
            "63000\n",
            "64000\n",
            "65000\n",
            "66000\n",
            "67000\n",
            "68000\n",
            "69000\n",
            "70000\n",
            "71000\n",
            "72000\n",
            "73000\n",
            "74000\n",
            "75000\n",
            "76000\n",
            "77000\n",
            "78000\n",
            "79000\n",
            "80000\n",
            "81000\n",
            "82000\n",
            "83000\n",
            "84000\n",
            "85000\n",
            "86000\n",
            "87000\n",
            "88000\n",
            "89000\n",
            "90000\n",
            "91000\n",
            "92000\n",
            "93000\n",
            "94000\n",
            "95000\n",
            "96000\n",
            "97000\n",
            "98000\n",
            "99000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqblHw6DUZUG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tg_test_size = len(tg_samples) // 10\n",
        "lenta_test_size = len(lenta_samples) // 10\n",
        "tg_test_samples = tg_samples[-tg_test_size:]\n",
        "train_samples = tg_samples[:-tg_test_size] + lenta_samples[:-lenta_test_size]\n",
        "test_samples = tg_test_samples + lenta_samples[-lenta_test_size:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9J3kXsXfUy7m",
        "colab_type": "code",
        "outputId": "af614f1a-e9ec-41af-d236-731202323684",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from sklearn import metrics\n",
        "from scipy import spatial\n",
        "\n",
        "scores = []\n",
        "test_y = []\n",
        "for sample in test_samples:\n",
        "    left_vector, right_vector, _, _, y = sample\n",
        "    test_y.append(y)\n",
        "    scores.append(-spatial.distance.cosine(left_vector, right_vector))\n",
        "metrics.roc_auc_score(test_y, scores)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7911066452652451"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrWCJwlcVBhH",
        "colab_type": "code",
        "outputId": "7fc829a1-8242-4aae-b3d4-2147f961e5ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "source": [
        "from keras.layers import Input, Dense, Dot\n",
        "from keras.models import Model\n",
        "\n",
        "left_input = Input(shape=(150,), dtype='float32')\n",
        "right_input = Input(shape=(150,), dtype='float32')\n",
        "dense = Dense(50, activation='linear')\n",
        "left_dense = dense(left_input)\n",
        "right_dense = dense(right_input)\n",
        "dot_layer = Dense(1, activation='sigmoid')(Dot(axes=1, normalize=True)([left_dense, right_dense]))\n",
        "nn_model = Model(inputs=[left_input, right_input], output=dot_layer)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LJ-X1NnVUcw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import optimizers\n",
        "nn_model.compile(optimizer=optimizers.Adam(lr=0.3), loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nq5WBl-yVXAb",
        "colab_type": "code",
        "outputId": "35060499-0ac0-401f-abb1-3e7cd44b6bdc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        }
      },
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
        "\n",
        "train_left = []\n",
        "train_right = []\n",
        "train_y = []\n",
        "for sample in train_samples:\n",
        "    left_vector, right_vector, left_text, right_text, y = sample\n",
        "    train_left.append(left_vector)\n",
        "    train_right.append(right_vector)\n",
        "    train_y.append(y)\n",
        "\n",
        "test_left = []\n",
        "test_right = []\n",
        "test_y = []\n",
        "for sample in test_samples:\n",
        "    left_vector, right_vector, _, _, y = sample\n",
        "    test_left.append(left_vector)\n",
        "    test_right.append(right_vector)\n",
        "    test_y.append(y)\n",
        "\n",
        "nn_model.fit([np.array(train_left), np.array(train_right)],\n",
        "             np.array(train_y),\n",
        "             batch_size=64,\n",
        "             epochs=100,\n",
        "             callbacks=[es,],\n",
        "             validation_data=([np.array(test_left), np.array(test_right)], np.array(test_y)),\n",
        "             verbose=2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 358772 samples, validate on 39862 samples\n",
            "Epoch 1/100\n",
            " - 22s - loss: 0.3045 - accuracy: 0.8525 - val_loss: 0.2445 - val_accuracy: 0.9035\n",
            "Epoch 2/100\n",
            " - 21s - loss: 0.1996 - accuracy: 0.9232 - val_loss: 0.2161 - val_accuracy: 0.9180\n",
            "Epoch 3/100\n",
            " - 20s - loss: 0.1879 - accuracy: 0.9281 - val_loss: 0.2063 - val_accuracy: 0.9264\n",
            "Epoch 4/100\n",
            " - 20s - loss: 0.1820 - accuracy: 0.9309 - val_loss: 0.2199 - val_accuracy: 0.9201\n",
            "Epoch 5/100\n",
            " - 20s - loss: 0.1775 - accuracy: 0.9330 - val_loss: 0.2039 - val_accuracy: 0.9266\n",
            "Epoch 6/100\n",
            " - 20s - loss: 0.1742 - accuracy: 0.9342 - val_loss: 0.1942 - val_accuracy: 0.9314\n",
            "Epoch 7/100\n",
            " - 20s - loss: 0.1718 - accuracy: 0.9353 - val_loss: 0.2044 - val_accuracy: 0.9274\n",
            "Epoch 8/100\n",
            " - 20s - loss: 0.1700 - accuracy: 0.9360 - val_loss: 0.1923 - val_accuracy: 0.9320\n",
            "Epoch 9/100\n",
            " - 20s - loss: 0.1681 - accuracy: 0.9368 - val_loss: 0.1966 - val_accuracy: 0.9303\n",
            "Epoch 10/100\n",
            " - 20s - loss: 0.1677 - accuracy: 0.9370 - val_loss: 0.1999 - val_accuracy: 0.9293\n",
            "Epoch 11/100\n",
            " - 20s - loss: 0.1655 - accuracy: 0.9383 - val_loss: 0.2024 - val_accuracy: 0.9288\n",
            "Epoch 12/100\n",
            " - 20s - loss: 0.1644 - accuracy: 0.9385 - val_loss: 0.2023 - val_accuracy: 0.9317\n",
            "Epoch 13/100\n",
            " - 20s - loss: 0.1639 - accuracy: 0.9390 - val_loss: 0.1986 - val_accuracy: 0.9305\n",
            "Epoch 00013: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f54400fe470>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avMGX3wduUZx",
        "colab_type": "code",
        "outputId": "56b41d4a-8ef2-4162-e1ff-b0b08ccd4a6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "source": [
        "embedder = Model(inputs=[left_input, ], output=left_dense)\n",
        "tg_test_left = []\n",
        "tg_test_right = []\n",
        "test_y = []\n",
        "for sample in tg_test_samples:\n",
        "    tg_left, tg_right, _, _, y = sample\n",
        "    tg_test_left.append(tg_left)\n",
        "    tg_test_right.append(tg_right)\n",
        "    test_y.append(y)\n",
        "pred_left = embedder.predict([np.array(tg_test_left)])\n",
        "pred_right = embedder.predict([np.array(tg_test_right)])\n",
        "scores = []\n",
        "for left, right in zip(pred_left, pred_right):\n",
        "    left = left / np.linalg.norm(left)\n",
        "    right = right / np.linalg.norm(right)\n",
        "    score = (left.dot(right) + 1.0) / 2.0 - 1.0\n",
        "    scores.append(score)\n",
        "metrics.roc_auc_score(test_y, scores)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9647562981754766"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkHxYX-EewGq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "matrix = dense.get_weights()[0]\n",
        "bias = dense.get_weights()[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClPcY2QYexbS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"matrix.txt\", \"w\") as w:\n",
        "    for row_num in range(matrix.shape[1]):\n",
        "        row = []\n",
        "        for col_num in range(matrix.shape[0]):\n",
        "            row.append(float(matrix[col_num][row_num]))\n",
        "        w.write(\",\".join(map(str, row)) + \"\\n\")\n",
        "\n",
        "with open(\"bias.txt\", \"w\") as w:\n",
        "    for value in bias:\n",
        "        w.write(\"{}\\n\".format(value))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}